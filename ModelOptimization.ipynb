{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efeda577",
   "metadata": {},
   "source": [
    "# Approach\n",
    "- Explore techniques to reduce the size of a trained huggingface.co transformers model\n",
    "    - serialization\n",
    "      - ONNX\n",
    "      - TorchScript\n",
    "    - quantization\n",
    "      - native PyTorch\n",
    "      - ONNX Runtime\n",
    "    - pruning\n",
    "    - change model architecture (requires retraining)\n",
    "- Measure and compare runtime and RAM usage for each technique\n",
    "- Measure change in model performance metrics for a sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e240bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "import optimize_models, model_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbcde2",
   "metadata": {},
   "source": [
    "## Baseline - roberta-base HuggingFace model\n",
    "### Run inference with memory profiling and create plots\n",
    "Note: An extra 0.4s wait time is added to model loading to ensure that memory due to model load (rather than inference) is measured properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c29b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_memory(model_path):\n",
    "    # https://pypi.org/project/memory-profiler/\n",
    "    model_name = model_path.replace(\"/\", \"_\")\n",
    "    model_name = model_name.replace(\".\", \"\")\n",
    "    command = \"mprof run --interval 0.1 inference_profiling.py \" + model_path + \"; mprof plot -o plots/\" + model_name + \".png -w 0,16\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a6762df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e068e7",
   "metadata": {},
   "source": [
    "# Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda9cda",
   "metadata": {},
   "source": [
    "## TorchScript Serialization\n",
    "PyTorch models can be compiled into TorchScript using either tracing or scripting. During tracing, sample input is fed into the trained model and followed (traced) through the model computation graph, which is then frozen. HuggingFace seems to currently only support tracing.\n",
    "\n",
    "### A note about AWS deployment with Elastic Inference:\n",
    "Currently, the only model types that are supported are MXNet and TorchScript. It is lame that ONNX isn't supported, as this seems more commonly used than MXNet or Torchscript.\n",
    "\n",
    "### ONNX --> MXNet\n",
    "There is a fairly opaque way to convert ONNX models to MXNet...\n",
    "\n",
    "Except that a bunch of operators that we need aren't even supported yet (ConstantOfShape, CumSum) - they are tagged as \"feature requests\" in the GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1823d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/acunliffe/git/model-optimization/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/acunliffe/git/model-optimization/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py:2154: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert all(\n"
     ]
    }
   ],
   "source": [
    "## PyTorch --> TorchScript\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer_name = \"roberta-base\"\n",
    "optimize_models.to_torchscript(model_name, tokenizer_name, \"torchscript\", \"tracing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c98ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"torchscript/tracing.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e610cc",
   "metadata": {},
   "source": [
    "## ONNX Serialization\n",
    "ONNX (Open Neural Network Exchange) is an open standard format for representing ML models. Deep learning models like transformers are converted to a computation graph.\n",
    "\n",
    "ONNX Runtime is maintained by Microsoft and provides tools for running inference and training with ONNX models. It also provides tools for optimizing models, e.g., quantization.\n",
    "\n",
    "***Note: onnxruntime-tools contains tools for optimizing transformers-based models (https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers; https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb). I have not experimented with these tools yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b797ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "token_type_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/transformers/blob/af8afdc88dcb07261acf70aee75f2ad00a4208a4/src/transformers/convert_graph_to_onnx.py\n",
    "# roberta-base\n",
    "optimize_models.to_onnx(\"roberta-base\", \"roberta-base\", \"onnx/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81583b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"onnx/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87021",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "\n",
    "Convert floating point (32-bit precision) to int8. There are 3 different types of quantization:\n",
    "\n",
    "https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\n",
    "\n",
    "https://onnxruntime.ai/docs/how-to/quantization.html\n",
    "\n",
    "1. Dynamic quantization: Convert weights to int8, convert activations to int8 prior to compute (but store as floating point).\n",
    "\n",
    "2. Static quantization: int8 arithmetic (like dynamic quantization), but also int8 memory access\n",
    "\n",
    "3. Quantization-aware training (QAT): During training, weights and activations are \"fake quantized\" during forward an backwards passes, meaning that computations are performed with floating point numbers, but float values are rounded to mimic int8 values for forward/backward passes.\n",
    "\n",
    "QAT is the most accurate form of quantization, followed by dynamic, then static. \n",
    "\n",
    "Static quantization will run slightly faster than dynamic and use less compute.\n",
    "\n",
    "From the ONNX documentation: \"In general, it is recommended to use dynamic quantization for RNN and transformer-based models, and static quantization for CNN models.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9e312",
   "metadata": {},
   "source": [
    "## PyTorch dynamic quantization\n",
    "\n",
    "Quantize the linear layers of a pytorch model, save as TorchScript model using torch.jit.trace\n",
    "\n",
    "Note: No CUDA support currently, model inference should be performed on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f855509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "optimize_models.quantize_pytorch_model(\"roberta-base\", \"roberta-base\", \"quantized-int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b425dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"quantized-int8/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfece25a",
   "metadata": {},
   "source": [
    "## ONNX dynamic quantization\n",
    "\n",
    "https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb\n",
    "\n",
    "Quantizes both the linear and embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2912f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_models.quantize_onnx_model(\"onnx/model.onnx\", \"onnx/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a8a8080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"onnx/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa59696",
   "metadata": {},
   "source": [
    "# Pruning\n",
    "\n",
    "## Magnitude pruning: Discard weights with low absolute values.\n",
    "This approach is most effective for models trained from scratch for a specific task since the values of the weights dictate importance for the task the model was trained on.\n",
    "In transfer learning however, this method is not as effective, since the values of the weights are more related to the task used to pre-train the network rather than the fine-tuning task.\n",
    "\n",
    "## Movement pruning: Discard weights that decrease in absolute value during training.\n",
    "This is more appropriate for the transfer learning/fine-tuning task - weights that shrink during training are not important for the fine-tuning task (their large values were actually counterproductive). These weights may be removed irrespective of their absolute value.\n",
    "\n",
    "Movement Pruning: Adaptive Sparsity by Fine-Tuning\n",
    "Victor Sanh et al., Hugging Face, Cornell\n",
    "October 2020\n",
    "Movement pruning demonstrates better ability to adapt to the end-task.\n",
    "95% of the original BERT performance with only 5% of the encoder's weight on NLI and question-answering.\n",
    "\n",
    "## Pruning heads\n",
    "Are 16 Heads Really Better than One?\n",
    "Michel et al., 2019\n",
    "https://arxiv.org/abs/1905.10650\n",
    "Models that are trained with many heads can be pruned at inference time without significantly affecting performance.\n",
    "Compute relative importance of attention heads and prune the least important heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b342c13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# pruning heads - randomly prune some heads to get a sense of the impact on inference time and RAM usage:\n",
    "base_model = \"roberta-base\"\n",
    "optimize_models.prune_random_heads(base_model, 0.25, \"pruned/25percent\")\n",
    "optimize_models.prune_random_heads(base_model, 0.5, \"pruned/50percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8de965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"pruned/25percent\")\n",
    "profile_memory(\"pruned/50percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b700c",
   "metadata": {},
   "source": [
    "# Change Model Architecture (requires re-training)\n",
    "\n",
    "DistilBERT: https://arxiv.org/abs/1910.01108\n",
    "October 2019 - Demonstrated similar performance as BERT on GLUE benchmark dataset at 40% of the size.\n",
    "\n",
    "DistilRoBERTa: 95% of RoBERTa-base's performance on GLUE, twice as fast as RoBERTa while being 35% smaller.\n",
    "\n",
    "How does it work?\n",
    "\n",
    "Knowledge Distillation / student-teacher: \"student\" - a smaller model - is trained to reproduce the behavior of a \"teacher\" - a larger model or ensemble of modles.\n",
    "\n",
    "Triple loss function:\n",
    "\n",
    "1. masked language modeling (MLM) objective\n",
    "2. distillation loss - similarity between output probability distribution of student and teacher models\n",
    "3. cosine distance similarity between student and teacher hidden states\n",
    "\n",
    "### Question: Can we do our own student-teacher set-up?\n",
    "\n",
    "Answer: Yes, it should be possible, though would require some custom code. It's probably better to start by trying to fine-tune a classification model from distilroberta-base rather than roberta-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e28986ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a53e8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "token_type_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# distilroberta with pruning (50%), serialization, and dynamic quantization\n",
    "optimize_models.prune_random_heads(\"distilroberta-base\", 0.5, \"distilroberta-pruned/50percent\")\n",
    "optimize_models.quantize_pytorch_model(\"distilroberta-pruned/50percent\", \"roberta-base\", \"distilroberta-quantized\")\n",
    "optimize_models.to_onnx(\"distilroberta-pruned/50percent\", \"roberta-base\", \"distilroberta-onnx/model.onnx\")\n",
    "optimize_models.quantize_onnx_model(\"distilroberta-onnx/model.onnx\", \"distilroberta-onnx/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c1ac941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n",
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"distilroberta-onnx/quantized-model.onnx\")\n",
    "profile_memory(\"distilroberta-quantized/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac8c52",
   "metadata": {},
   "source": [
    "# Examine Model Performance Metrics\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "\n",
    "## Task: sentiment analysis using a model available on huggingface.co, based on roberta-base, trained on tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35f6f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_memory_measure_performance(model_path):\n",
    "    # https://pypi.org/project/memory-profiler/\n",
    "    model_name = model_path.replace(\"cardiffnlp/twitter-roberta-base-sentiment\", \"baseline-sentiment\")\n",
    "    model_name = model_path.replace(\"/\", \"_\")\n",
    "    model_name = model_name.replace(\".\", \"\")\n",
    "    command = \"mprof run --interval 0.1 sentiment_inference.py \" + model_path + \"; mprof plot -o plots/\" + model_name + \".png -w 0,16\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15928d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:12<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.7\n",
      "\n",
      "confusion matrix\n",
      "[[19 10  0]\n",
      " [13 37  5]\n",
      " [ 1  1 14]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "# Measure baseline model performance metrics\n",
    "profile_memory_measure_performance(\"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d96827",
   "metadata": {},
   "source": [
    "## Model Serialization (shouldn't change model performance metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372a088",
   "metadata": {},
   "source": [
    "### ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "248e0f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "token_type_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "task = \"sentiment\"\n",
    "model_name = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "tokenizer_name = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "optimize_models.to_onnx(model_name, tokenizer_name, \"onnx-sentiment/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff34faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.7\n",
      "\n",
      "confusion matrix\n",
      "[[19 10  0]\n",
      " [13 37  5]\n",
      " [ 1  1 14]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory_measure_performance(\"onnx-sentiment/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9326a2",
   "metadata": {},
   "source": [
    "### TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1efaa2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "task = \"sentiment\"\n",
    "model_name = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "tokenizer_name = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "optimize_models.to_torchscript(model_name, tokenizer_name, \"torchscript-sentiment\", \"tracing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d328b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.7\n",
      "\n",
      "confusion matrix\n",
      "[[19 10  0]\n",
      " [13 37  5]\n",
      " [ 1  1 14]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory_measure_performance(\"torchscript-sentiment/tracing.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd1b106",
   "metadata": {},
   "source": [
    "## ONNX quantization (should change model performance metrics somewhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7355afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_models.quantize_onnx_model(\"onnx-sentiment/model.onnx\", \"onnx-sentiment/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3217ac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.69\n",
      "\n",
      "confusion matrix\n",
      "[[19  9  1]\n",
      " [13 37  5]\n",
      " [ 1  2 13]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory_measure_performance(\"onnx-sentiment/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ab6f2",
   "metadata": {},
   "source": [
    "## pytorch native quantization (should change model performance metrics somewhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20f877de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "task = \"sentiment\"\n",
    "model_name = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "tokenizer_name = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "optimize_models.quantize_pytorch_model(model_name, tokenizer_name, \"quantized-int8-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc6a25f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.7\n",
      "\n",
      "confusion matrix\n",
      "[[18 11  0]\n",
      " [12 39  4]\n",
      " [ 0  3 13]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory_measure_performance(\"quantized-int8-sentiment/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddc66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
