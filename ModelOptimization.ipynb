{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efeda577",
   "metadata": {},
   "source": [
    "# Approach\n",
    "- Explore techniques to reduce the size of a trained huggingface.co transformers model\n",
    "    - serialization\n",
    "      - ONNX\n",
    "      - TorchScript\n",
    "    - quantization\n",
    "      - native PyTorch\n",
    "      - ONNX Runtime\n",
    "    - pruning\n",
    "    - change model architecture (requires retraining)\n",
    "- Measure and compare runtime and RAM usage for each technique\n",
    "- Measure change in model performance metrics for a sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e240bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "import optimize_models\n",
    "from model_utils import save_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbcde2",
   "metadata": {},
   "source": [
    "## Baseline - roberta-base HuggingFace model\n",
    "### Run inference with memory profiling and create plots\n",
    "Note: An extra 0.4s wait time is added to model loading to ensure that memory due to model load (rather than inference) is measured properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c29b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_memory(model_path):\n",
    "    # https://pypi.org/project/memory-profiler/\n",
    "    model_name = model_path.replace(\"/\", \"_\")\n",
    "    model_name = model_name.replace(\".\", \"\")\n",
    "    command = \"mprof run --interval 0.1 inference_profiling.py \" + model_path + \"; mprof plot -o plots/\" + model_name + \".png -w 0,30\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907fb95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# save a roberta-base model for sequence classification - Note that this model will have\n",
    "# random weights in the penultimate layer (since we haven't trained the base model\n",
    "# for the classification task), making the actual predictions useless. The timing and memory\n",
    "# consumption metrics, however, are still valid even if the predictions aren't.\n",
    "save_model_and_tokenizer(\"roberta-base\", \"models/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6762df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"models/roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e068e7",
   "metadata": {},
   "source": [
    "# Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda9cda",
   "metadata": {},
   "source": [
    "## TorchScript Serialization\n",
    "PyTorch models can be compiled into TorchScript using either tracing or scripting. During tracing, sample input is fed into the trained model and followed (traced) through the model computation graph, which is then frozen. HuggingFace seems to currently only support tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1823d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PyTorch --> TorchScript\n",
    "model_name = \"models/roberta-base\"\n",
    "tokenizer_name = \"models/roberta-base\"\n",
    "optimize_models.to_torchscript(model_name, tokenizer_name, \"models/torchscript\", \"tracing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c98ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"models/torchscript/tracing.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e610cc",
   "metadata": {},
   "source": [
    "## ONNX Serialization\n",
    "ONNX (Open Neural Network Exchange) is an open standard format for representing ML models. Deep learning models like transformers are converted to a computation graph.\n",
    "\n",
    "ONNX Runtime is maintained by Microsoft and provides tools for running inference and training with ONNX models. It also provides tools for optimizing models, e.g., quantization.\n",
    "\n",
    "***Note: onnxruntime-tools contains tools for optimizing transformers-based models (https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers; https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb). I have not experimented with these tools yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b797ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.10.2\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/Users/acunliffe/git/model-optimization/.venv/lib/python3.9/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/Users/acunliffe/git/model-optimization/.venv/lib/python3.9/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'logits'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 2) matches (2, 2)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "All good, model saved at: models/onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/transformers/blob/af8afdc88dcb07261acf70aee75f2ad00a4208a4/src/transformers/convert_graph_to_onnx.py\n",
    "# roberta-base\n",
    "optimize_models.to_onnx(\"models/roberta-base\", \"models/onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81583b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"models/onnx/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87021",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "\n",
    "Convert floating point (32-bit precision) to int8. There are 3 different types of quantization:\n",
    "\n",
    "https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\n",
    "\n",
    "https://onnxruntime.ai/docs/how-to/quantization.html\n",
    "\n",
    "1. Dynamic quantization: Convert weights to int8, convert activations to int8 prior to compute (but store as floating point).\n",
    "\n",
    "2. Static quantization: int8 arithmetic (like dynamic quantization), but also int8 memory access\n",
    "\n",
    "3. Quantization-aware training (QAT): During training, weights and activations are \"fake quantized\" during forward an backwards passes, meaning that computations are performed with floating point numbers, but float values are rounded to mimic int8 values for forward/backward passes.\n",
    "\n",
    "QAT is the most accurate form of quantization, followed by dynamic, then static. \n",
    "\n",
    "Static quantization will run slightly faster than dynamic and use less compute.\n",
    "\n",
    "From the ONNX documentation: \"In general, it is recommended to use dynamic quantization for RNN and transformer-based models, and static quantization for CNN models.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9e312",
   "metadata": {},
   "source": [
    "## PyTorch dynamic quantization\n",
    "\n",
    "Quantize the linear layers of a pytorch model, save as TorchScript model using torch.jit.trace\n",
    "\n",
    "Note: No CUDA support currently, model inference should be performed on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f855509",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_models.quantize_pytorch_model(\"models/roberta-base\", \"models/roberta-base\", \"models/quantized-int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b425dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"models/quantized-int8/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfece25a",
   "metadata": {},
   "source": [
    "## ONNX dynamic quantization\n",
    "\n",
    "https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb\n",
    "\n",
    "Quantizes both the linear and embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2912f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_models.quantize_onnx_model(\"models/onnx/model.onnx\", \"models/onnx/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a8a8080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"models/onnx/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa59696",
   "metadata": {},
   "source": [
    "# Pruning\n",
    "\n",
    "## Magnitude pruning: Discard weights with low absolute values.\n",
    "This approach is most effective for models trained from scratch for a specific task since the values of the weights dictate importance for the task the model was trained on.\n",
    "In transfer learning however, this method is not as effective, since the values of the weights are more related to the task used to pre-train the network rather than the fine-tuning task.\n",
    "\n",
    "## Movement pruning: Discard weights that decrease in absolute value during training.\n",
    "This is more appropriate for the transfer learning/fine-tuning task - weights that shrink during training are not important for the fine-tuning task (their large values were actually counterproductive). These weights may be removed irrespective of their absolute value.\n",
    "\n",
    "Movement Pruning: Adaptive Sparsity by Fine-Tuning\n",
    "Victor Sanh et al., Hugging Face, Cornell\n",
    "October 2020\n",
    "Movement pruning demonstrates better ability to adapt to the end-task.\n",
    "95% of the original BERT performance with only 5% of the encoder's weight on NLI and question-answering.\n",
    "\n",
    "## Pruning heads\n",
    "Are 16 Heads Really Better than One?\n",
    "Michel et al., 2019\n",
    "https://arxiv.org/abs/1905.10650\n",
    "Models that are trained with many heads can be pruned at inference time without significantly affecting performance.\n",
    "Compute relative importance of attention heads and prune the least important heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b342c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning heads - randomly prune some heads to get a sense of the impact on inference time and RAM usage:\n",
    "base_model = \"models/roberta-base\"\n",
    "optimize_models.prune_random_heads(base_model, 0.25, \"models/pruned/25percent\")\n",
    "optimize_models.prune_random_heads(base_model, 0.5, \"models/pruned/50percent\")\n",
    "optimize_models.prune_random_heads(base_model, 0.9, \"models/pruned/90percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8de965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using last profile data.\n",
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using last profile data.\n",
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"models/pruned/25percent\")\n",
    "profile_memory(\"models/pruned/50percent\")\n",
    "profile_memory(\"models/pruned/90percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b700c",
   "metadata": {},
   "source": [
    "# Change Model Architecture (requires re-training)\n",
    "\n",
    "DistilBERT: https://arxiv.org/abs/1910.01108\n",
    "October 2019 - Demonstrated similar performance as BERT on GLUE benchmark dataset at 40% of the size.\n",
    "\n",
    "DistilRoBERTa: 95% of RoBERTa-base's performance on GLUE, twice as fast as RoBERTa while being 35% smaller.\n",
    "\n",
    "How does it work?\n",
    "\n",
    "Knowledge Distillation / student-teacher: \"student\" - a smaller model - is trained to reproduce the behavior of a \"teacher\" - a larger model or ensemble of modles.\n",
    "\n",
    "Triple loss function:\n",
    "\n",
    "1. masked language modeling (MLM) objective\n",
    "2. distillation loss - similarity between output probability distribution of student and teacher models\n",
    "3. cosine distance similarity between student and teacher hidden states\n",
    "\n",
    "### Question: Can we do our own student-teacher set-up?\n",
    "\n",
    "Answer: Yes, it should be possible, though would require some custom code. It's probably better to start by trying to fine-tune a classification model from distilroberta-base rather than roberta-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "822beda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "save_model_and_tokenizer(\"distilroberta-base\", \"models/distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e28986ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"models/distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilroberta with serialization and dynamic quantization\n",
    "optimize_models.to_onnx(\"models/distilroberta-base\", \"models/distilroberta-onnx\")\n",
    "optimize_models.quantize_onnx_model(\"models/distilroberta-onnx/model.onnx\", \"models/distilroberta-onnx/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilroberta with pruning (90%), serialization, and dynamic quantization\n",
    "optimize_models.prune_random_heads(\"models/distilroberta-base\", 0.9, \"models/distilroberta-pruned/90percent\")\n",
    "optimize_models.to_onnx(\"models/distilroberta-pruned/90percent\", \"models/distilroberta-onnx-pruned\")\n",
    "optimize_models.quantize_onnx_model(\"models/distilroberta-onnx-pruned/model.onnx\", \"models/distilroberta-onnx-pruned/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c1ac941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory(\"models/distilroberta-onnx/quantized-model.onnx\")\n",
    "profile_memory(\"models/distilroberta-onnx-pruned/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac8c52",
   "metadata": {},
   "source": [
    "# Examine Model Performance Metrics\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "\n",
    "## Task: sentiment analysis using a model available on huggingface.co, based on roberta-base, trained on tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35f6f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_memory_measure_performance(model_path):\n",
    "    # https://pypi.org/project/memory-profiler/\n",
    "    model_name = model_path.replace(\"/\", \"_\")\n",
    "    model_name = model_name.replace(\".\", \"\")\n",
    "    command = \"mprof run --interval 0.1 sentiment_inference.py \" + model_path + \"; mprof plot -o plots/\" + model_name + \".png -w 0,16\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ededb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_tokenizer(\"cardiffnlp/twitter-roberta-base-sentiment\", \"models/baseline-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15928d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:13<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.7\n",
      "\n",
      "confusion matrix\n",
      "[[19 10  0]\n",
      " [13 37  5]\n",
      " [ 1  1 14]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "# Measure baseline model performance metrics\n",
    "profile_memory_measure_performance(\"models/baseline-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d96827",
   "metadata": {},
   "source": [
    "## Model Serialization (shouldn't change model performance metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372a088",
   "metadata": {},
   "source": [
    "### ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "248e0f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.10.2\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/Users/acunliffe/git/model-optimization/.venv/lib/python3.9/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/Users/acunliffe/git/model-optimization/.venv/lib/python3.9/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'logits'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 3) matches (2, 3)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "All good, model saved at: models/onnx-sentiment/model.onnx\n"
     ]
    }
   ],
   "source": [
    "optimize_models.to_onnx(\"models/baseline-sentiment\", \"models/onnx-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ff34faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.7\n",
      "\n",
      "confusion matrix\n",
      "[[19 10  0]\n",
      " [13 37  5]\n",
      " [ 1  1 14]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory_measure_performance(\"models/onnx-sentiment/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9326a2",
   "metadata": {},
   "source": [
    "### TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1efaa2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"models/baseline-sentiment\"\n",
    "optimize_models.to_torchscript(model_name, model_name, \"models/torchscript-sentiment\", \"tracing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d328b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.7\n",
      "\n",
      "confusion matrix\n",
      "[[19 10  0]\n",
      " [13 37  5]\n",
      " [ 1  1 14]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory_measure_performance(\"models/torchscript-sentiment/tracing.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd1b106",
   "metadata": {},
   "source": [
    "## ONNX quantization (should change model performance metrics somewhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7355afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_models.quantize_onnx_model(\"models/onnx-sentiment/model.onnx\", \"models/onnx-sentiment/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3217ac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.69\n",
      "\n",
      "confusion matrix\n",
      "[[19  9  1]\n",
      " [13 37  5]\n",
      " [ 1  2 13]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory_measure_performance(\"models/onnx-sentiment/quantized-model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ab6f2",
   "metadata": {},
   "source": [
    "## pytorch native quantization (should change model performance metrics somewhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20f877de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"models/baseline-sentiment\"\n",
    "optimize_models.quantize_pytorch_model(model_name, model_name, \"models/quantized-int8-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc6a25f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mprof: Sampling memory every 0.1s\n",
      "running new process\n",
      "running as a Python program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "----- MODEL PERFORMANCE METRICS -----\n",
      "\n",
      "accuracy 0.71\n",
      "\n",
      "confusion matrix\n",
      "[[18 11  0]\n",
      " [10 40  5]\n",
      " [ 0  3 13]]\n",
      "\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "\n",
      "Using last profile data.\n"
     ]
    }
   ],
   "source": [
    "profile_memory_measure_performance(\"models/quantized-int8-sentiment/model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
